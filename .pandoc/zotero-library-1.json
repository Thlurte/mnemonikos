[{"id":"barnettSevenFailurePoints2024","abstract":"Software engineers are increasingly adding semantic search capabilities to applications using a strategy known as Retrieval Augmented Generation (RAG). A RAG system involves finding documents that semantically match a query and then passing the documents to a large language model (LLM) such as ChatGPT to extract the right answer using an LLM. RAG systems aim to: a) reduce the problem of hallucinated responses from LLMs, b) link sources/references to generated responses, and c) remove the need for annotating documents with meta-data. However, RAG systems suffer from limitations inherent to information retrieval systems and from reliance on LLMs. In this paper, we present an experience report on the failure points of RAG systems from three case studies from separate domains: research, education, and biomedical. We share the lessons learned and present 7 failure points to consider when designing a RAG system. The two key takeaways arising from our work are: 1) validation of a RAG system is only feasible during operation, and 2) the robustness of a RAG system evolves rather than designed in at the start. We conclude with a list of potential research directions on RAG systems for the software engineering community.","accessed":{"date-parts":[["2024",1,31]]},"author":[{"family":"Barnett","given":"Scott"},{"family":"Kurniawan","given":"Stefanus"},{"family":"Thudumu","given":"Srikanth"},{"family":"Brannelly","given":"Zach"},{"family":"Abdelrazek","given":"Mohamed"}],"citation-key":"barnettSevenFailurePoints2024","issued":{"date-parts":[["2024",1,11]]},"number":"arXiv:2401.05856","publisher":"arXiv","source":"arXiv.org","title":"Seven Failure Points When Engineering a Retrieval Augmented Generation System","type":"article","URL":"http://arxiv.org/abs/2401.05856"},{"id":"cuconasuPowerNoiseRedefining2024","abstract":"Retrieval-Augmented Generation (RAG) systems represent a significant advancement over traditional Large Language Models (LLMs). RAG systems enhance their generation ability by incorporating external data retrieved through an Information Retrieval (IR) phase, overcoming the limitations of standard LLMs, which are restricted to their pre-trained knowledge and limited context window. Most research in this area has predominantly concentrated on the generative aspect of LLMs within RAG systems. Our study fills this gap by thoroughly and critically analyzing the influence of IR components on RAG systems. This paper analyzes which characteristics a retriever should possess for an effective RAG's prompt formulation, focusing on the type of documents that should be retrieved. We evaluate various elements, such as the relevance of the documents to the prompt, their position, and the number included in the context. Our findings reveal, among other insights, that including irrelevant documents can unexpectedly enhance performance by more than 30% in accuracy, contradicting our initial assumption of diminished quality. These findings call for developing specialized approaches tailored to the specific demands of integrating retrieval with language generation models and pave the way for future research. These results underscore the need for developing specialized strategies to integrate retrieval with language generation models, thereby laying the groundwork for future research in this field.","accessed":{"date-parts":[["2024",1,29]]},"author":[{"family":"Cuconasu","given":"Florin"},{"family":"Trappolini","given":"Giovanni"},{"family":"Siciliano","given":"Federico"},{"family":"Filice","given":"Simone"},{"family":"Campagnano","given":"Cesare"},{"family":"Maarek","given":"Yoelle"},{"family":"Tonellotto","given":"Nicola"},{"family":"Silvestri","given":"Fabrizio"}],"citation-key":"cuconasuPowerNoiseRedefining2024","issued":{"date-parts":[["2024",1,26]]},"number":"arXiv:2401.14887","publisher":"arXiv","source":"arXiv.org","title":"The Power of Noise: Redefining Retrieval for RAG Systems","title-short":"The Power of Noise","type":"article","URL":"http://arxiv.org/abs/2401.14887"},{"id":"kochSiameseNeuralNetworks2015","author":[{"family":"Koch","given":"Gregory R."}],"citation-key":"kochSiameseNeuralNetworks2015","issued":{"date-parts":[["2015"]]},"title":"Siamese Neural Networks for One-Shot Image Recognition","type":"paper-conference","URL":"https://api.semanticscholar.org/CorpusID:13874643"},{"id":"krizhevskyImageNetClassificationDeep2012","author":[{"family":"Krizhevsky","given":"Alex"},{"family":"Sutskever","given":"Ilya"},{"family":"Hinton","given":"Geoffrey E"}],"citation-key":"krizhevskyImageNetClassificationDeep2012","container-title":"Advances in Neural Information Processing Systems","editor":[{"family":"Pereira","given":"F."},{"family":"Burges","given":"C. J."},{"family":"Bottou","given":"L."},{"family":"Weinberger","given":"K. Q."}],"issued":{"date-parts":[["2012"]]},"publisher":"Curran Associates, Inc.","title":"ImageNet Classification with Deep Convolutional Neural Networks","type":"paper-conference","URL":"https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf","volume":"25"},{"id":"lecunGradientbasedLearningApplied1998","accessed":{"date-parts":[["2024",1,25]]},"author":[{"family":"Lecun","given":"Y."},{"family":"Bottou","given":"L."},{"family":"Bengio","given":"Y."},{"family":"Haffner","given":"P."}],"citation-key":"lecunGradientbasedLearningApplied1998","container-title":"Proceedings of the IEEE","container-title-short":"Proc. IEEE","DOI":"10.1109/5.726791","ISSN":"00189219","issue":"11","issued":{"literal":"Nov./1998"},"page":"2278-2324","source":"DOI.org (Crossref)","title":"Gradient-based learning applied to document recognition","type":"article-journal","URL":"http://ieeexplore.ieee.org/document/726791/","volume":"86"},{"id":"lewisRetrievalAugmentedGenerationKnowledgeIntensive2021","abstract":"Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.","accessed":{"date-parts":[["2024",1,29]]},"author":[{"family":"Lewis","given":"Patrick"},{"family":"Perez","given":"Ethan"},{"family":"Piktus","given":"Aleksandra"},{"family":"Petroni","given":"Fabio"},{"family":"Karpukhin","given":"Vladimir"},{"family":"Goyal","given":"Naman"},{"family":"Küttler","given":"Heinrich"},{"family":"Lewis","given":"Mike"},{"family":"Yih","given":"Wen-tau"},{"family":"Rocktäschel","given":"Tim"},{"family":"Riedel","given":"Sebastian"},{"family":"Kiela","given":"Douwe"}],"citation-key":"lewisRetrievalAugmentedGenerationKnowledgeIntensive2021","issued":{"date-parts":[["2021",4,12]]},"number":"arXiv:2005.11401","publisher":"arXiv","source":"arXiv.org","title":"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks","type":"article","URL":"http://arxiv.org/abs/2005.11401"},{"id":"liRedTeamingVisual2024","abstract":"VLMs (Vision-Language Models) extend the capabilities of LLMs (Large Language Models) to accept multimodal inputs. Since it has been verified that LLMs can be induced to generate harmful or inaccurate content through specific test cases (termed as Red Teaming), how VLMs perform in similar scenarios, especially with their combination of textual and visual inputs, remains a question. To explore this problem, we present a novel red teaming dataset RTVLM, which encompasses 10 subtasks (e.g., image misleading, multi-modal jail-breaking, face fairness, etc) under 4 primary aspects (faithfulness, privacy, safety, fairness). Our RTVLM is the first red-teaming dataset to benchmark current VLMs in terms of these 4 different aspects. Detailed analysis shows that 10 prominent open-sourced VLMs struggle with the red teaming in different degrees and have up to 31% performance gap with GPT-4V. Additionally, we simply apply red teaming alignment to LLaVA-v1.5 with Supervised Fine-tuning (SFT) using RTVLM, and this bolsters the models' performance with 10% in RTVLM test set, 13% in MM-Hal, and without noticeable decline in MM-Bench, overpassing other LLaVA-based models with regular alignment data. This reveals that current open-sourced VLMs still lack red teaming alignment. Our code and datasets will be open-source.","accessed":{"date-parts":[["2024",1,29]]},"author":[{"family":"Li","given":"Mukai"},{"family":"Li","given":"Lei"},{"family":"Yin","given":"Yuwei"},{"family":"Ahmed","given":"Masood"},{"family":"Liu","given":"Zhenguang"},{"family":"Liu","given":"Qi"}],"citation-key":"liRedTeamingVisual2024","issued":{"date-parts":[["2024",1,23]]},"number":"arXiv:2401.12915","publisher":"arXiv","source":"arXiv.org","title":"Red Teaming Visual Language Models","type":"article","URL":"http://arxiv.org/abs/2401.12915"},{"id":"wanKnowledgeFusionLarge2024","abstract":"While training large language models (LLMs) from scratch can generate models with distinct functionalities and strengths, it comes at significant costs and may result in redundant capabilities. Alternatively, a cost-effective and compelling approach is to merge existing pre-trained LLMs into a more potent model. However, due to the varying architectures of these LLMs, directly blending their weights is impractical. In this paper, we introduce the notion of knowledge fusion for LLMs, aimed at combining the capabilities of existing LLMs and transferring them into a single LLM. By leveraging the generative distributions of source LLMs, we externalize their collective knowledge and unique strengths, thereby potentially elevating the capabilities of the target model beyond those of any individual source LLM. We validate our approach using three popular LLMs with different architectures--Llama-2, MPT, and OpenLLaMA--across various benchmarks and tasks. Our findings confirm that the fusion of LLMs can improve the performance of the target model across a range of capabilities such as reasoning, commonsense, and code generation. Our code, model weights, and data are public at \\url{https://github.com/fanqiwan/FuseLLM}.","accessed":{"date-parts":[["2024",1,29]]},"author":[{"family":"Wan","given":"Fanqi"},{"family":"Huang","given":"Xinting"},{"family":"Cai","given":"Deng"},{"family":"Quan","given":"Xiaojun"},{"family":"Bi","given":"Wei"},{"family":"Shi","given":"Shuming"}],"citation-key":"wanKnowledgeFusionLarge2024","issued":{"date-parts":[["2024",1,22]]},"number":"arXiv:2401.10491","publisher":"arXiv","source":"arXiv.org","title":"Knowledge Fusion of Large Language Models","type":"article","URL":"http://arxiv.org/abs/2401.10491"},{"id":"RunLLMStreaming","type":"webpage","title":"Run an LLM in a streaming pipeline  |  Cloud Dataflow  |  Google Cloud","URL":"https://cloud.google.com/dataflow/docs/tutorials/streaming-llm","accessed":{"date-parts":[["2024",2,6]]},"citation-key":"RunLLMStreaming","library":"My Library","citekey":"RunLLMStreaming"},{"id":"RunLLMStreaminga","type":"webpage","abstract":"This tutorial shows how to run a large language model (LLM) in a streaming Dataflow pipeline by using the Apache Beam RunInference API.","container-title":"Google Cloud","language":"en","title":"Run an LLM in a streaming pipeline | Cloud Dataflow","URL":"https://cloud.google.com/dataflow/docs/tutorials/streaming-llm","accessed":{"date-parts":[["2024",2,6]]},"citation-key":"RunLLMStreaminga","library":"My Library","citekey":"RunLLMStreaminga"}]