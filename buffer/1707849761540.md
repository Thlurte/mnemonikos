The generation of incorrect information, or ”hallucinations” ,  is  when queries extend beyond the model’s training data or necessitate up-to-date information. [^1][^2]

[^1]: Y. Zhang _et al._, “Siren’s Song in the AI Ocean: A Survey on Hallucination in Large Language Models.” arXiv, Sep. 24, 2023. Available: [http://arxiv.org/abs/2309.01219](http://arxiv.org/abs/2309.01219). [Accessed: Feb. 13, 2024]

[^2]: Y. Gao _et al._, “Retrieval-Augmented Generation for Large Language Models: A Survey.” arXiv, Jan. 04, 2024. Available: [http://arxiv.org/abs/2312.10997](http://arxiv.org/abs/2312.10997). [Accessed: Feb. 07, 2024]
