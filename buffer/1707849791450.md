The Naive RAG follows a traditional process that includes indexing, retrieval, and generation. It is also characterized as a “Retrieve-Read” framework.[^1]

Indexing begins with data indexing, where original data is cleansed and extracted, and various file formats such as PDF, HTML, Word, and Markdown are converted into standardized plain text. In order to fit within the context limitations of language models, this text is then segmented into smaller, more manageable chunks in a process known as chunking. These chunks are subsequently transformed into vector representations through an embedding model, chosen for its balance between inference efficiency and model size. This facilitates similarity comparisons during the retrieval phase. Finally, an index is created to store these text chunks and their vector embeddings as key-value pairs, which allows for efficient and scalable search capabilities. [^2]

Upon receipt of a user query, the system employs the same encoding model utilized during the indexing phase to transcode the input into a vector representation. It then proceeds to compute the similarity scores between the query vector and the vectorized chunks within the indexed corpus. The system prioritizes and retrieves the top K chunks that demonstrate the greatest similarity to the query. These chunks are subsequently used as the expanded contextual basis for addressing the user’s request. [^2]

The posed query and selected documents are synthesized into a coherent prompt to which a large language model is tasked with formulating a response. The model’s approach to answering may vary depending on task-specific criteria, allowing it to either draw upon its inherent parametric knowledge or restrict its responses to the information contained within the provided documents. [^2]



[^1]: X. Ma, Y. Gong, P. He, H. Zhao, and N. Duan, “Query Rewriting for Retrieval-Augmented Large Language Models.” arXiv, Oct. 22, 2023. Available: [http://arxiv.org/abs/2305.14283](http://arxiv.org/abs/2305.14283). [Accessed: Feb. 13, 2024]


[^2]: Y. Gao _et al._, “Retrieval-Augmented Generation for Large Language Models: A Survey.” arXiv, Jan. 04, 2024. Available: [http://arxiv.org/abs/2312.10997](http://arxiv.org/abs/2312.10997). [Accessed: Feb. 07, 2024]
