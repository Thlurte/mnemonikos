Modular RAG integrates various methods to enhance functional modules, such as incorporating a search module for similarity retrieval and applying a fine-tuning approach in the retriever.  [^1]

Restructured RAG modules [^2] and iterative methodologies [^3]  have been developed to address specific issues in modular RAG.

*Search Module*: Search Module is tailored to specific scenarios and incorporates direct searches on additional corpora. This integration is achieved using code generated by the LLM, query languages such as SQL or Cypher, and other custom tools. The data sources for these searches can include search engines, text data, tabular data, and knowledge graphs.[^4]

*Memory Module*: The approach involves identifying memories most similar to the current input. Selfmem[^5] utilizes a retrieval-enhanced generator to create an unbounded memory pool iteratively, combining the “original question” and “dual question”.

*Fusion*: RAG-Fusion [^6] enhances traditional search systems by addressing their limitations through a multi-query approach that expands user queries into multiple, diverse perspectives using an LLM.
The fusion process involves parallel vector searches of both original and expanded queries, intelligent re-ranking to optimize results, and pairing the best outcomes with new queries.

*Routing*: The RAG system’s retrieval process utilizes diverse sources, differing in domain, language, and format, which can be either alternated or merged based on the situation[^7].

*Predict*: Predict addresses the common issues of redundancy and noise in retrieved content. Instead of directly retrieving from a data source, this module utilizes the LLM to generate the necessary context[^2]

*Task Adapter*: This module focuses on adapting RAG to a variety of downstream tasks. 

> [!example]
> UPRISE automates the retrieval of prompts for zero-shot task inputs from a pre-constructed data pool, thereby enhancing universality across tasks and models.[^8]

> [!example]
> PROMPTAGATOR utilizes LLM as a few-shot query generator and, based on the generated data, creates task-specific retrievers.[^9]

[^9]:Z. Dai _et al._, “Promptagator: Few-shot Dense Retrieval From 8 Examples.” arXiv, Sep. 23, 2022. Available: [http://arxiv.org/abs/2209.11755](http://arxiv.org/abs/2209.11755). [Accessed: Feb. 14, 2024]

[^8]:D. Cheng _et al._, “UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation.” arXiv, Dec. 16, 2023. Available: [http://arxiv.org/abs/2303.08518](http://arxiv.org/abs/2303.08518). [Accessed: Feb. 14, 2024]

[^7]:X. Li, E. Nie, and S. Liang, “From Classification to Generation: Insights into Crosslingual Retrieval Augmented ICL.” arXiv, Dec. 02, 2023. Available: [http://arxiv.org/abs/2311.06595](http://arxiv.org/abs/2311.06595). [Accessed: Feb. 14, 2024]

[^6]: Adrian H. Raudaschl. Forget rag, the future is rag-fusion. https://towardsdatascience.com/ forget-rag-the-future-is-rag-fusion-1147298d8ad1, 2023.

[^5]: X. Cheng, D. Luo, X. Chen, L. Liu, D. Zhao, and R. Yan, “Lift Yourself Up: Retrieval-augmented Text Generation with Self Memory.” arXiv, Dec. 23, 2023. Available: [http://arxiv.org/abs/2305.02437](http://arxiv.org/abs/2305.02437). [Accessed: Feb. 14, 2024]

[^4]:X. Wang _et al._, “KnowledGPT: Enhancing Large Language Models with Retrieval and Storage Access on Knowledge Bases.” arXiv, Aug. 17, 2023. Available: [http://arxiv.org/abs/2308.11761](http://arxiv.org/abs/2308.11761). [Accessed: Feb. 14, 2024]


[^3]:Z. Shao, Y. Gong, Y. Shen, M. Huang, N. Duan, and W. Chen, “Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy.” arXiv, Oct. 23, 2023. Available: [http://arxiv.org/abs/2305.15294](http://arxiv.org/abs/2305.15294). [Accessed: Feb. 14, 2024]

[^2]:W. Yu _et al._, “Generate rather than Retrieve: Large Language Models are Strong Context Generators.” arXiv, Jan. 25, 2023. Available: [http://arxiv.org/abs/2209.10063](http://arxiv.org/abs/2209.10063). [Accessed: Feb. 14, 2024] 

[^1]:X. V. Lin _et al._, “RA-DIT: Retrieval-Augmented Dual Instruction Tuning.” arXiv, Nov. 05, 2023. Available: [http://arxiv.org/abs/2310.01352](http://arxiv.org/abs/2310.01352). [Accessed: Feb. 14, 2024]



2024-02-14 Wednesday - 20:54:31
